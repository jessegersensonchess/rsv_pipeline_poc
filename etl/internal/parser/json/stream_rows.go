// Package jsonparser provides JSON parsing helpers and a streaming adapter used
// by the ETL pipeline. This file adds a streaming adapter that converts JSON
// records into *transformer.Row values suitable for the generic ETL pipeline
// in cmd/etl.
//
// High-level flow (mirrors the XML streaming adapter conceptually):
//
//  1. Decode JSON from an io.Reader using encoding/json.Decoder so we can
//     handle large inputs and JSONL/NDJSON-style streams.
//  2. Support the same envelope shape used by probeJSON:
//     - root array of objects: [ {...}, {...} ]
//     - root object with one or more array-of-object fields: { "records": [...] }
//     - single object: { ... } (treated as one record)
//  3. For each logical record (map[string]any):
//     - Apply parser.options.header_map (original → normalized) to build a
//     canonical map keyed by normalized names, exactly like the CSV path.
//     - Build a *transformer.Row whose V slice is ordered according to the
//     ETL storage columns slice.
//  4. Stream rows into the 'out' channel for downstream transform/validate/load.
package json

import (
	"context"
	"encoding/json"
	"fmt"
	"io"

	"etl/internal/config"
	"etl/internal/transformer"
)

// StreamJSONRows parses JSON from r according to parserOpts and streams records
// as *transformer.Row into 'out'.
//
// Contract:
//
//   - columns is the ordered list of destination columns. For each emitted
//     record, we construct a []any where row[i] corresponds to columns[i].
//     The transformer/loader stages rely on this alignment.
//
//   - parserOpts may contain a "header_map" object mapping original JSON keys
//     to normalized column names. This mirrors the CSV path, where probeJSON
//     generates a header_map in parser.options. We accept both map[string]any
//     (when coming from JSON) and map[string]string (when constructed in Go).
//
//   - The function is intentionally single-threaded from the ETL perspective:
//     it does not spawn its own worker pool. Concurrency is controlled at the
//     ETL layer via runtime.reader_workers.
//
//   - onParseErr is used to report fatal parse errors; per-record parse errors
//     are not distinguished in this initial implementation.
func StreamJSONRows(
	ctx context.Context,
	r io.Reader,
	columns []string,
	parserOpts config.Options,
	out chan<- *transformer.Row,
	onParseErr func(line int, err error),
) error {
	dec := json.NewDecoder(r)

	// Extract header_map from parser options, accepting both map[string]any and
	// map[string]string. We intentionally do NOT use Options.StringMap here,
	// because probeJSON builds a map[string]string directly in Go.
	headerMap := readHeaderMap(parserOpts)

	line := 0

	// emitObject takes a single JSON object (map[string]any), applies header_map
	// to canonicalize keys to normalized column names, and sends a *Row to 'out'.
	emitObject := func(obj map[string]any) error {
		line++

		// Apply header_map if present: build a canonical-key object so that
		// downstream lookup by normalized column names works for original keys
		// like "Identifikační číslo" → "identifikacni_cislo".
		canon := obj
		if len(headerMap) > 0 {
			canon = make(map[string]any, len(obj))
			for k, v := range obj {
				if mapped, ok := headerMap[k]; ok && mapped != "" {
					canon[mapped] = v
				} else {
					canon[k] = v
				}
			}
		}

		values := recordToRowJSON(canon, columns)
		row := &transformer.Row{
			Line: line,
			V:    values,
		}

		select {
		case out <- row:
			return nil
		case <-ctx.Done():
			return ctx.Err()
		}
	}

	// Decode first top-level value to determine shape: object, array, etc.
	var root any
	if err := dec.Decode(&root); err != nil {
		if err == io.EOF {
			return nil // empty input
		}
		if onParseErr != nil {
			onParseErr(0, err)
		}
		return fmt.Errorf("json: decode root: %w", err)
	}

	switch v := root.(type) {
	case []any:
		// Top-level array of objects: [ {...}, {...}, ... ]
		for _, elem := range v {
			obj, ok := elem.(map[string]any)
			if !ok {
				err := fmt.Errorf("json: array element not an object (got %T)", elem)
				if onParseErr != nil {
					onParseErr(line+1, err)
				}
				return err
			}
			if err := emitObject(obj); err != nil {
				return err
			}
		}

	case map[string]any:
		// Top-level object: either a single record or an envelope containing
		// the actual records in one of its array-of-object fields.
		if slice := findObjectSlice(v); slice != nil {
			for _, obj := range slice {
				if err := emitObject(obj); err != nil {
					return err
				}
			}
		} else {
			// Treat as a single record.
			if err := emitObject(v); err != nil {
				return err
			}
		}

	default:
		err := fmt.Errorf("json: unsupported root type %T (want object or array)", v)
		if onParseErr != nil {
			onParseErr(0, err)
		}
		return err
	}

	// Optional: handle additional top-level values (JSONL/NDJSON style).
	for {
		var obj map[string]any
		if err := dec.Decode(&obj); err != nil {
			if err == io.EOF {
				break
			}
			if onParseErr != nil {
				onParseErr(line+1, err)
			}
			return fmt.Errorf("json: decode subsequent value: %w", err)
		}
		if err := emitObject(obj); err != nil {
			return err
		}
	}

	return nil
}

// readHeaderMap extracts a header_map from parser options, accepting both
// map[string]any (typical when coming from JSON) and map[string]string (when
// constructed in Go code, e.g. by probeJSON).
func readHeaderMap(opts config.Options) map[string]string {
	res := make(map[string]string)

	raw := opts.Any("header_map")
	if raw == nil {
		return res
	}

	switch m := raw.(type) {
	case map[string]string:
		for k, v := range m {
			res[k] = v
		}
	case map[string]any:
		for k, v := range m {
			if s, ok := v.(string); ok {
				res[k] = s
			}
		}
	default:
		// Unsupported type; leave res empty.
	}

	return res
}

// findObjectSlice searches the top-level object for a value that is an
// array-of-object and returns the first such slice it finds.
//
// This mirrors the "envelope" behavior described in probeJSON: if you have a
// structure like:
//
//	{
//	  "records": [ { ... }, { ... } ],
//	  "meta":    { ... }
//	}
//
// then findObjectSlice will return the []map[string]any corresponding to the
// "records" array.
func findObjectSlice(root map[string]any) []map[string]any {
	for _, v := range root {
		rawSlice, ok := v.([]any)
		if !ok || len(rawSlice) == 0 {
			continue
		}
		objects := make([]map[string]any, 0, len(rawSlice))
		valid := true
		for _, elem := range rawSlice {
			if elem == nil {
				continue
			}
			m, ok := elem.(map[string]any)
			if !ok {
				valid = false
				break
			}
			objects = append(objects, m)
		}
		if valid && len(objects) > 0 {
			return objects
		}
	}
	return nil
}

// recordToRowJSON maps a JSON object (with canonical keys) into a []any aligned
// with the given columns slice. Missing keys become nil.
//
// This helper is intentionally small and deterministic; it is used by the ETL
// pipeline so that JSON and CSV/XML share the same "columns → row values"
// contract.
func recordToRowJSON(obj map[string]any, columns []string) []any {
	row := make([]any, len(columns))
	for i, col := range columns {
		row[i] = obj[col] // nil if missing
	}
	return row
}
